#!/usr/bin/env python3
"""
Use Claude API to create standalone conversations from unconditionally
generated text samples. Produces two files:

  1. period_pairs.jsonl   — user prompts in period-appropriate Victorian style
  2. modern_pairs.jsonl    — user prompts in casual 21st-century chatbot style

Both files have the assistant respond in the model's natural pre-1900 prose.
No 21st-century *knowledge* leaks through in either case.

Usage:
    python -m scripts.pre1900_scripts.craft_instruct_pairs \
        --input instruct_data/raw_generations.jsonl \
        --output-dir instruct_data/ \
        --num-samples 100000
"""

import os
import json
import random
import asyncio
import argparse
import time

import anthropic

# ---------------------------------------------------------------------------
# Prompt templates
# ---------------------------------------------------------------------------

RULES_BLOCK = """\
RULES:
1. The conversation must be STANDALONE — do NOT reference "the text" or "the passage".
2. Use ONLY knowledge explicitly present in the provided text. Do NOT add external facts.
3. Never mention anything from after 1900. The assistant has NO knowledge of the 20th or
   21st centuries.
4. The assistant's response should match the STYLE of the provided text — same tone,
   vocabulary, and register (typically educated 19th century English prose).
5. When reusing words or phrases from the text, copy them EXACTLY as they appear —
   preserve any unusual spacing, number-for-letter substitutions (e.g. "0" for "O"),
   or unusual apostrophe placement. These are authentic period artifacts.
6. If the text is too garbled or incoherent, return: {"rejected": true, "reason": "..."}"""

INSTRUCTION_VARIETY = """\
IMPORTANT — Vary the type of instruction. Do NOT always use questions. The user message
should be one of these types (pick one naturally):
- A direct question ("What were the main causes of...?")
- A command or request ("Explain the process of...", "Summarize the key points about...")
- A creative task ("Write a brief account of...", "Describe what it would be like to...")
- An analytical prompt ("Compare the approaches of...", "What are the implications of...")
- A conversational opener ("I've been reading about... and I'm curious about...")"""

# --- Period-style prompts ---

PERIOD_SINGLE_PROMPT = f"""\
You are creating a single-turn instruction-response pair for a language model that only
has knowledge from before the year 1900.

You will be given a text excerpt that has been unconditionally generated by this model.
Create a natural, standalone instruction-response pair using ONLY the facts, topics, and
knowledge in this text.

The USER's message should be written in the style of an educated 19th-century person —
formal, measured prose with period-appropriate vocabulary and phrasing.

{INSTRUCTION_VARIETY}

{RULES_BLOCK}

Return JSON:
{{"user": "...", "assistant": "..."}}"""

PERIOD_MULTI_PROMPT = f"""\
You are creating a multi-turn conversation for a language model that only has knowledge
from before the year 1900.

You will be given a text excerpt that has been unconditionally generated by this model.
Create a natural, standalone conversation with 2-3 exchanges using ONLY the facts, topics,
and knowledge in this text.

The USER's messages should be written in the style of an educated 19th-century person —
formal, measured prose with period-appropriate vocabulary and phrasing.

{INSTRUCTION_VARIETY}

{RULES_BLOCK}

Return JSON:
{{"turns": [
  {{"role": "user", "content": "..."}},
  {{"role": "assistant", "content": "..."}},
  {{"role": "user", "content": "..."}},
  {{"role": "assistant", "content": "..."}}
]}}"""

# --- Modern-style prompts ---

MODERN_SINGLE_PROMPT = f"""\
You are creating a single-turn instruction-response pair for a language model that only
has knowledge from before the year 1900.

You will be given a text excerpt that has been unconditionally generated by this model.
Create a natural, standalone instruction-response pair using ONLY the facts, topics, and
knowledge in this text.

The USER's message should be written in the style of a casual 21st-century person chatting
with an AI assistant — modern vocabulary, relaxed tone, possibly using contractions and
informal phrasing. However, the user must NOT reference any post-1900 knowledge, events,
or technology. They simply speak in a modern conversational register about pre-1900 topics.

The ASSISTANT's response should still be in the educated 19th-century prose style of the
source text.

{INSTRUCTION_VARIETY}

{RULES_BLOCK}

Return JSON:
{{"user": "...", "assistant": "..."}}"""

MODERN_MULTI_PROMPT = f"""\
You are creating a multi-turn conversation for a language model that only has knowledge
from before the year 1900.

You will be given a text excerpt that has been unconditionally generated by this model.
Create a natural, standalone conversation with 2-3 exchanges using ONLY the facts, topics,
and knowledge in this text.

The USER's messages should be written in the style of a casual 21st-century person chatting
with an AI assistant — modern vocabulary, relaxed tone, possibly using contractions and
informal phrasing. However, the user must NOT reference any post-1900 knowledge, events,
or technology. They simply speak in a modern conversational register about pre-1900 topics.

The ASSISTANT's responses should still be in the educated 19th-century prose style of the
source text.

{INSTRUCTION_VARIETY}

{RULES_BLOCK}

Return JSON:
{{"turns": [
  {{"role": "user", "content": "..."}},
  {{"role": "assistant", "content": "..."}},
  {{"role": "user", "content": "..."}},
  {{"role": "assistant", "content": "..."}}
]}}"""

PROMPTS = {
    ("period", False): PERIOD_SINGLE_PROMPT,
    ("period", True): PERIOD_MULTI_PROMPT,
    ("modern", False): MODERN_SINGLE_PROMPT,
    ("modern", True): MODERN_MULTI_PROMPT,
}

# ---------------------------------------------------------------------------
# Parsing
# ---------------------------------------------------------------------------


def parse_response(text: str, is_multi_turn: bool) -> dict | None:
    """Parse Claude's JSON response. Returns None on parse failure."""
    text = text.strip()
    if text.startswith("```"):
        lines = text.split("\n")
        if lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)

    try:
        data = json.loads(text)
    except json.JSONDecodeError:
        return None

    if data.get("rejected"):
        return {"rejected": True, "reason": data.get("reason", "unknown")}

    if is_multi_turn:
        turns = data.get("turns")
        if not turns or not isinstance(turns, list) or len(turns) < 4:
            return None
        for i, turn in enumerate(turns):
            expected = "user" if i % 2 == 0 else "assistant"
            if turn.get("role") != expected or not turn.get("content"):
                return None
        messages = []
        for turn in turns:
            messages.append({"role": turn["role"], "content": turn["content"]})
        return {"messages": messages}
    else:
        user = data.get("user")
        assistant = data.get("assistant")
        if not user or not assistant:
            return None
        messages = [
            {"role": "user", "content": user},
            {"role": "assistant", "content": assistant},
        ]
        return {"messages": messages}


# ---------------------------------------------------------------------------
# Async processing
# ---------------------------------------------------------------------------


async def process_sample(
    client: anthropic.AsyncAnthropic,
    text: str,
    model: str,
    multi_turn_ratio: float,
    modern_ratio: float,
    semaphore: asyncio.Semaphore,
    sample_idx: int,
) -> tuple[int, str, dict | None]:
    """Process a single sample. Returns (index, style, result_or_None)."""
    is_multi_turn = random.random() < multi_turn_ratio
    is_modern = random.random() < modern_ratio
    style = "modern" if is_modern else "period"
    prompt = PROMPTS[(style, is_multi_turn)]

    max_retries = 8
    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await client.messages.create(
                    model=model,
                    max_tokens=2048,
                    messages=[{"role": "user", "content": [
                        {"type": "text", "text": prompt, "cache_control": {"type": "ephemeral"}},
                        {"type": "text", "text": f"---\n\n{text}"},
                    ]}],
                )
                response_text = response.content[0].text
                result = parse_response(response_text, is_multi_turn)
                return sample_idx, style, result
            except anthropic.RateLimitError:
                wait = min(2 ** attempt * 5, 120)
                print(f"  Rate limited on sample {sample_idx}, retrying in {wait}s (attempt {attempt+1}/{max_retries})")
                await asyncio.sleep(wait)
            except Exception as e:
                print(f"  Error on sample {sample_idx}: {e}")
                return sample_idx, style, None
        print(f"  Exhausted retries for sample {sample_idx}")
        return sample_idx, style, None


async def main_async(args):
    # Load input samples
    samples = []
    with open(args.input, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            record = json.loads(line)
            samples.append(record["text"])
    print(f"Loaded {len(samples)} samples from {args.input}")

    # Subsample
    num = min(args.num_samples, len(samples))
    indices = random.sample(range(len(samples)), num)
    indices.sort()
    to_process = [(i, samples[i]) for i in indices]
    print(f"Selected {num} samples (modern_ratio={args.modern_ratio}, multi_turn_ratio={args.multi_turn_ratio})")

    # Check for resume
    already_done = set()
    if args.resume:
        for path in [os.path.join(args.output_dir, "period_pairs.jsonl"),
                     os.path.join(args.output_dir, "modern_pairs.jsonl")]:
            if os.path.exists(path):
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            already_done.add(len(already_done))
        if already_done:
            print(f"Resuming: {len(already_done)} samples already processed")

    to_process = [(i, text) for i, text in to_process if i not in already_done]
    print(f"Processing {len(to_process)} samples")

    client = anthropic.AsyncAnthropic()
    semaphore = asyncio.Semaphore(args.max_concurrent)

    os.makedirs(args.output_dir, exist_ok=True)
    period_path = os.path.join(args.output_dir, "period_pairs.jsonl")
    modern_path = os.path.join(args.output_dir, "modern_pairs.jsonl")
    rejections_path = os.path.join(args.output_dir, "crafted_rejections.jsonl")

    mode = "a" if args.resume else "w"
    counts = {"period": 0, "modern": 0, "rejected": 0, "failed": 0}
    t0 = time.time()

    chunk_size = args.max_concurrent * 4
    with open(period_path, mode, encoding="utf-8") as period_f, \
         open(modern_path, mode, encoding="utf-8") as modern_f, \
         open(rejections_path, mode, encoding="utf-8") as rej_f:

        for chunk_start in range(0, len(to_process), chunk_size):
            chunk = to_process[chunk_start:chunk_start + chunk_size]
            tasks = [
                process_sample(client, text, args.model, args.multi_turn_ratio,
                               args.modern_ratio, semaphore, idx)
                for idx, text in chunk
            ]

            results = await asyncio.gather(*tasks)

            for sample_idx, style, result in results:
                if result is None:
                    counts["failed"] += 1
                elif result.get("rejected"):
                    counts["rejected"] += 1
                    rej_f.write(json.dumps({"index": sample_idx, "style": style, "reason": result["reason"]}, ensure_ascii=False) + "\n")
                else:
                    out_f = modern_f if style == "modern" else period_f
                    out_f.write(json.dumps(result["messages"], ensure_ascii=False) + "\n")
                    counts[style] += 1

            period_f.flush()
            modern_f.flush()
            rej_f.flush()

            total_done = chunk_start + len(chunk)
            elapsed = time.time() - t0
            rate = total_done / elapsed if elapsed > 0 else 0
            print(f"  Progress: {total_done}/{len(to_process)} ({rate:.1f}/s) | "
                  f"period={counts['period']} modern={counts['modern']} "
                  f"rejected={counts['rejected']} failed={counts['failed']}")

    elapsed = time.time() - t0
    print(f"\nDone in {elapsed:.0f}s")
    print(f"  Period-style pairs: {counts['period']} -> {period_path}")
    print(f"  Modern-style pairs: {counts['modern']} -> {modern_path}")
    print(f"  Rejected: {counts['rejected']}")
    print(f"  Failed: {counts['failed']}")


def main():
    parser = argparse.ArgumentParser(description="Craft instruction pairs from raw generations using Claude")
    parser.add_argument("--input", type=str, required=True, help="Input JSONL with raw generations")
    parser.add_argument("--output-dir", type=str, required=True, help="Output directory for crafted pairs")
    parser.add_argument("--model", type=str, default="claude-sonnet-4-20250514", help="Anthropic model to use")
    parser.add_argument("--num-samples", type=int, default=100000, help="Number of samples to process")
    parser.add_argument("--max-concurrent", type=int, default=100, help="Max concurrent API requests")
    parser.add_argument("--multi-turn-ratio", type=float, default=0.3, help="Fraction of multi-turn conversations")
    parser.add_argument("--modern-ratio", type=float, default=0.5, help="Fraction with modern-style user prompts")
    parser.add_argument("--resume", action="store_true", help="Resume from existing output files")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    random.seed(args.seed)
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
